{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d7d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Import the actual Source model for correct typing\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from app.models.chat_models import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4773b4",
   "metadata": {},
   "source": [
    "# Demonstrating LangChain Usage in RAGService\n",
    "\n",
    "This notebook demonstrates how LangChain is utilized in the `RAGService` class for AI chat functionality with context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab4c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Mock VectorStoreService\n",
    "class MockVectorStoreService:\n",
    "    async def search_similar_notes(self, query: str, n_results: int):\n",
    "        return [\n",
    "            Source(\n",
    "                note_id=1,\n",
    "                title=\"Note 1\", \n",
    "                content_snippet=\"This is a snippet of note 1 containing important information about AI and machine learning concepts.\",\n",
    "                relevance_score=0.9,\n",
    "                created_at=datetime.now(),\n",
    "                updated_at=datetime.now()\n",
    "            ),\n",
    "            Source(\n",
    "                note_id=2,\n",
    "                title=\"Note 2\", \n",
    "                content_snippet=\"This is a snippet of note 2 with details about software development and programming best practices.\",\n",
    "                relevance_score=0.8,\n",
    "                created_at=datetime.now(),\n",
    "                updated_at=datetime.now()\n",
    "            )\n",
    "        ]\n",
    "\n",
    "mock_vector_store_service = MockVectorStoreService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d1d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RAGService Instance\n",
    "from app.services.rag_service import RAGService\n",
    "\n",
    "rag_service = RAGService(vector_store_service=mock_vector_store_service)\n",
    "\n",
    "# Mock initialization (skipping actual API key setup for demonstration)\n",
    "import asyncio\n",
    "await rag_service.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e46f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: Note 1 provides important information about AI and machine learning concepts. It covers essential topics related to artificial intelligence and machine learning, offering a foundational understanding of these technologies. If you have any specific questions or need more details from Note 1, feel free to ask!\n",
      "Sources used: 2\n",
      "  1. Note 1 (score: 0.90)\n",
      "  2. Note 2 (score: 0.80)\n"
     ]
    }
   ],
   "source": [
    "# Simulate a Chat Interaction\n",
    "async def simulate_chat():\n",
    "    user_message = \"What is the summary of note 1?\"\n",
    "    response = await rag_service.chat(message=user_message)\n",
    "    print(\"Assistant Response:\", response.response)\n",
    "    print(\"Sources used:\", len(response.sources))\n",
    "    for i, source in enumerate(response.sources):\n",
    "        print(f\"  {i+1}. {source.title} (score: {source.relevance_score:.2f})\")\n",
    "\n",
    "await simulate_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d907a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated System Prompt:\n",
      " You are an AI assistant helping users find and discuss information from their personal notes. \n",
      "\n",
      "Your role:\n",
      "- Answer questions based on the provided note context\n",
      "- Be helpful, accurate, and conversational\n",
      "- If the context doesn't contain enough information, say so honestly\n",
      "- Always cite which notes you're referencing when possible\n",
      "- Summarize and synthesize information from multiple notes when relevant\n",
      "\n",
      "Guidelines:\n",
      "- Be concise but thorough\n",
      "- Use a friendly, personal tone since these are the user's own notes\n",
      "- If asked about something not in the notes, clarify that you can only work with the provided notes\n",
      "- When referencing notes, mention the note title when possible\n",
      "\n",
      "Relevant notes context:\n",
      "\n",
      "1. Note: \"Note 1\"\n",
      "   Content: This is a snippet of note 1 about AI and machine learning.\n",
      "   Relevance: 0.90\n",
      "\n",
      "2. Note: \"Note 2\"\n",
      "   Content: This is a snippet of note 2 about software development.\n",
      "   Relevance: 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect System Prompt Generation\n",
    "sources = [\n",
    "    Source(\n",
    "        note_id=1,\n",
    "        title=\"Note 1\", \n",
    "        content_snippet=\"This is a snippet of note 1 about AI and machine learning.\",\n",
    "        relevance_score=0.9,\n",
    "        created_at=datetime.now(),\n",
    "        updated_at=datetime.now()\n",
    "    ),\n",
    "    Source(\n",
    "        note_id=2,\n",
    "        title=\"Note 2\", \n",
    "        content_snippet=\"This is a snippet of note 2 about software development.\",\n",
    "        relevance_score=0.8,\n",
    "        created_at=datetime.now(),\n",
    "        updated_at=datetime.now()\n",
    "    )\n",
    "]\n",
    "system_prompt = rag_service._create_system_prompt(sources)\n",
    "print(\"Generated System Prompt:\\n\", system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c90868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Context: [HumanMessage(content='What is note 1 about?'), AIMessage(content='Note 1 is about AI and ML.')]\n"
     ]
    }
   ],
   "source": [
    "# Review Conversation History Management\n",
    "conversation_id = rag_service._get_or_create_conversation(None)\n",
    "rag_service._add_message_to_conversation(conversation_id, \"user\", \"What is note 1 about?\")\n",
    "rag_service._add_message_to_conversation(conversation_id, \"assistant\", \"Note 1 is about AI and ML.\")\n",
    "\n",
    "conversation_context = rag_service._get_conversation_context(conversation_id)\n",
    "print(\"Conversation Context:\", conversation_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae7e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Information ===\n",
      "LLM Type: ChatOpenAI\n",
      "Model Name: gpt-3.5-turbo\n",
      "Temperature: 0.7\n",
      "Max Tokens: 1000\n",
      "\n",
      "=== Service Stats ===\n",
      "total_conversations: 2\n",
      "total_messages: 4\n",
      "max_context_notes: 5\n",
      "conversation_window: 10\n",
      "model: gpt-3.5-turbo\n",
      "\n",
      "=== Environment Configuration ===\n",
      "OPENAI_MODEL: gpt-3.5-turbo\n",
      "OPENAI_TEMPERATURE: 0.7\n",
      "OPENAI_API_KEY: Set\n"
     ]
    }
   ],
   "source": [
    "# Check which LLM is being used\n",
    "print(\"=== LLM Information ===\")\n",
    "\n",
    "# Method 1: Check the LLM object directly\n",
    "if rag_service.llm:\n",
    "    print(f\"LLM Type: {type(rag_service.llm).__name__}\")\n",
    "    print(f\"Model Name: {getattr(rag_service.llm, 'model_name', 'Unknown')}\")\n",
    "    print(f\"Temperature: {getattr(rag_service.llm, 'temperature', 'Unknown')}\")\n",
    "    print(f\"Max Tokens: {getattr(rag_service.llm, 'max_tokens', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"LLM not initialized\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Method 2: Use the service stats method\n",
    "stats = rag_service.get_service_stats()\n",
    "print(\"=== Service Stats ===\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Method 3: Check environment variables that control LLM selection\n",
    "print(\"=== Environment Configuration ===\")\n",
    "print(f\"OPENAI_MODEL: {os.getenv('OPENAI_MODEL', 'Not set (defaults to gpt-3.5-turbo)')}\")\n",
    "print(f\"OPENAI_TEMPERATURE: {os.getenv('OPENAI_TEMPERATURE', 'Not set (defaults to 0.7)')}\")\n",
    "print(f\"OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else 'Not set'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01290044",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated how LangChain is used in the `RAGService` class for AI chat functionality, including system prompt generation, conversation history management, and interaction with a mock LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
